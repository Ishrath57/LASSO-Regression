---
title: "Sample Report - Data Science Capstone"
author: "Ishrath"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is LASSO Regression?

LASSO (Least Absolute Shrinkage and Selection Operator) regression is a technique used for regularization and variable selection in machine learning and statistics. By placing a limit on the model parameters and requiring the total of the absolute values of the regression coefficients to be smaller than a predetermined value (λ), it seeks to minimize the prediction error. Due to this restriction, the solution is sparse, meaning that some regression coefficients are reduced to a value of zero, hence removing the associated variables from the model. Automated k-fold cross-validation, which divides the dataset into k subsamples, uses k-1 subsamples for model construction, and validates the model on the remaining subsample, is commonly used to determine the choice of λ. After carrying out this procedure k times, the pooled validation results are used to determine the preferred λ. [@J2018]


Combining shrinkage and variable selection for linear regression is known as LASSO (Least Absolute Shrinkage and Selection Operator) regression. This is accomplished by including an L1 penalty term, which reduces several feature coefficients to zero and enables automated variable, model, and parameter selection. The LASSO regression model is being utilized more and more in brain modeling, text classification utilizing tidy data principles, and medical diagnosis to predict disease outcomes and side effects. By placing restrictions on the model's parameters and causing the regression coefficients to decrease toward zero, it seeks to determine the variables and regression coefficients that minimize prediction error. This technique works very well with models that have a lot of multicollinearity and has been used in a variety of domains, such as genetic data.[@C2022]


### Related work

This section is going to cover the literature review...

## Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

## References

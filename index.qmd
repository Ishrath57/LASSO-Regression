---
title: "Data Science Capstone"
author: "Ishrath"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is LASSO Regression?

LASSO (Least Absolute Shrinkage and Selection Operator) regression is a technique used for regularization and variable selection in machine learning and statistics. By placing a limit on the model parameters and requiring the total of the absolute values of the regression coefficients to be smaller than a predetermined value (λ), it seeks to minimize the prediction error. Due to this restriction, the solution is sparse, meaning that some regression coefficients are reduced to a value of zero, hence removing the associated variables from the model. Automated k-fold cross-validation, which divides the dataset into k subsamples, uses k-1 subsamples for model construction, and validates the model on the remaining subsample, is commonly used to determine the choice of λ. After carrying out this procedure k times, the pooled validation results are used to determine the preferred λ. [@J2018]


Combining shrinkage and variable selection for linear regression is known as LASSO (Least Absolute Shrinkage and Selection Operator) regression. This is accomplished by including an L1 penalty term, which reduces several feature coefficients to zero and enables automated variable, model, and parameter selection. The LASSO regression model is being utilized more and more in brain modeling, text classification utilizing tidy data principles, and medical diagnosis to predict disease outcomes and side effects. By placing restrictions on the model's parameters and causing the regression coefficients to decrease toward zero, it seeks to determine the variables and regression coefficients that minimize prediction error. This technique works very well with models that have a lot of multicollinearity and has been used in a variety of domains, such as genetic data.[@C2022]

Using the LASSO (Least Absolute Shrinkage and Selection Operator) regression method for economic indicator modeling is the aim of the project "Advance statistical modeling of economic indicators: A LASSO regression approach". Because LASSO is a statistical technique that does both variable selection and regularization, it can be used to reduce overfitting and pick key variables that improve the accuracy of economic models. The project's goal is to use LASSO's feature selection and regularization skills to create a statistical model for economic indicators that is more precise and effective. When it comes to economic indicator modeling, the LASSO approach is well-known for its capacity to manage high-dimensional data and enhance forecast accuracy through the selection of pertinent variables and the reduction of regression coefficients.[@Emmert]

The use of the LASSO (Least Absolute Shrinkage and Selection Operator) regression approach in advanced statistical modeling of economic indicators is important for several reasons:

Variable Selection and Regularization:
In economic indicator modeling, where the impact of some factors may be more substantial than that of others, LASSO aids in the selection of the most pertinent variables and the exclusion of the less significant ones.

Handling Multicollinearity:
By reducing the coefficients of linked variables, LASSO helps manage multicollinearity, a prevalent problem in economic data, and provide more stable and understandable models.[@Emmert]

Prediction Accuracy: 
The accuracy of predictive models for economic indicators can be increased via LASSO by minimizing overfitting and concentrating on the most important variables.

Methods:
The LASSO (Least Absolute Shrinkage and Selection Operator) method is solved using a regularization process that involves minimizing the sum of the squared error term and the absolute magnitude of the regression coefficients. This is achieved through the following methods:

1.	Mathematical Formulation: The LASSO method solves the optimization problem by minimizing the residual sum of squares subject to the constraint that the sum of the absolute values of the coefficients is less than a constant.

2.	Coordinate Descent Algorithm: One common method for solving the LASSO problem is the coordinate descent algorithm, which iteratively updates the coefficients by fixing all but one coefficient at each step.

3.	Least Angle Regression (LARS): LARS is another method used to solve the LASSO problem, particularly for high-dimensional data. It proceeds in a direction equiangular to each of the predictors and has the advantage of being computationally efficient.[@R-base]


Result/Limitations:
Inability for Large Values: 
For large values of the explanatory variables, Lasso models—including the linear one—cannot be accurate or even somewhat accurate.

Difficulty in Interpreting Significance:
Since Lasso does not explicitly provide p-values for hypothesis testing, it might be difficult to establish which variables are statistically significant.[@R-base]



When the total of the absolute values of the coefficients is less than a constant, the "lasso" reduces the residual sum of squares. This constraint tends to yield certain coefficients that are exactly 0, which results in models that are understandable. Based on our simulation simulations, it appears that the lasso benefits from certain advantageous aspects of both ridge regression and subset selection. It demonstrates the stability of ridge regression and generates interpretable models such as subset selection. Additionally, there's an intriguing connection to Donoho and Johnstone's recent work on adaptive function estimation. The lasso concept is quite generic and can be used in many different statistical models: brief descriptions are given of expansions to generalized regression models and tree-based models.[@J2021]

Tibshirani (1996) proposed the least absolute shrinkage and selection operator (LASSO) for simultaneous parameter estimation and variable (model) selection in regression analysis. A specific instance of penalized least squares regression using the L1-penalty function is the LASSO.
The LASSO estimate can be defined by
$$
\begin{equation*} \hat{\beta}^{lasso}=\underset{\beta}{\arg\min}\left\{\frac{1}{2}\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j}\right)^{2}+\lambda\sum_{j=1}^{p}\left\vert\beta_{j}\right\vert\right\} \tag{2} \end{equation*} $$

which can also be written as
$$ \begin{align*} \hat{\beta}^{lasso}= & \underset{\beta}{\arg\min}\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j}\right)^{2},\\ & \quad\text{subject to}\ \sum_{j=1}^{p}\left\vert\beta_{j}\right\vert\leq t, \end{align*}
$$
LASSO truncates at zero and transforms all coefficients by a constant component λ. It is therefore a prospective variable selection technique for regression. As long as the total of the coefficients' absolute values is less than a constant, it reduces the residual sum of squares. Although LASSO was first developed in relation to least squares, it can be used to a broad range of models..[@788]

By combining the advantages of subset selection with ridge regression, LASSO enhances both prediction accuracy and model interpretability. When the set of predictors has a high correlation, LASSO selects one and reduces the others to zero. By precisely decreasing some of the coefficients to zero, it produces models that are easy to understand and lowers the variability of the estimations.[@788]

### Related work

This section is going to cover the literature review...

## Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

## References
